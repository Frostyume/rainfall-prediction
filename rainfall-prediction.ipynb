{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-10T16:28:34.946266Z",
     "iopub.status.busy": "2025-03-10T16:28:34.946033Z",
     "iopub.status.idle": "2025-03-10T16:29:32.797033Z",
     "shell.execute_reply": "2025-03-10T16:29:32.796057Z",
     "shell.execute_reply.started": "2025-03-10T16:28:34.946244Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install cmaes\n",
    "# !pip install category_encoders\n",
    "# !pip install -U imbalanced-learn\n",
    "# !pip install torch\n",
    "# !pip install pytorch-tabnet\n",
    "\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime,time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "from prettytable import PrettyTable\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(style='darkgrid', font_scale=1.4)\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "tqdm_notebook.get_lock().locks = []\n",
    "# !pip install sweetviz\n",
    "# import sweetviz as sv\n",
    "import concurrent.futures\n",
    "from copy import deepcopy       \n",
    "from functools import partial\n",
    "from itertools import combinations\n",
    "import random\n",
    "from random import randint, uniform\n",
    "import gc\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler,PowerTransformer, FunctionTransformer\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from itertools import combinations\n",
    "from sklearn.impute import SimpleImputer\n",
    "import xgboost as xg\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.metrics import mean_squared_error,mean_squared_log_error, roc_auc_score, accuracy_score, f1_score, precision_recall_curve, log_loss\n",
    "from sklearn.cluster import KMeans\n",
    "# !pip install yellowbrick\n",
    "# from yellowbrick.cluster import KElbowVisualizer\n",
    "# !pip install gap-stat\n",
    "# from gap_statistic.optimalK import OptimalK\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import boxcox\n",
    "import math\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "# !pip install optuna\n",
    "import optuna\n",
    "\n",
    "import cmaes\n",
    "import xgboost as xgb\n",
    "# !pip install catboost\n",
    "# !pip install lightgbm --install-option=--gpu --install-option=\"--boost-root=C:/local/boost_1_69_0\" --install-option=\"--boost-librarydir=C:/local/boost_1_69_0/lib64-msvc-14.1\"\n",
    "import lightgbm as lgb\n",
    "\n",
    "from category_encoders import OneHotEncoder, OrdinalEncoder, CountEncoder, CatBoostEncoder\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier, GradientBoostingClassifier,ExtraTreesClassifier, AdaBoostClassifier\n",
    "\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from catboost import CatBoost, CatBoostRegressor, CatBoostClassifier\n",
    "from sklearn.svm import NuSVC, SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifierCV, LogisticRegressionCV\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "from catboost import Pool\n",
    "import re\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\n",
    "import tensorflow\n",
    "import keras\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Dense, Activation, LeakyReLU, PReLU, ELU, Dropout\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from colorama import Fore, Style\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.pandas.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T16:29:32.798763Z",
     "iopub.status.busy": "2025-03-10T16:29:32.798078Z",
     "iopub.status.idle": "2025-03-10T16:29:32.900339Z",
     "shell.execute_reply": "2025-03-10T16:29:32.899002Z",
     "shell.execute_reply.started": "2025-03-10T16:29:32.798733Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# GPU设备检测\n",
    "global device\n",
    "gpus = tensorflow.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"GPU is available\")\n",
    "    device = 'gpu'  # 启用GPU加速\n",
    "else:\n",
    "    print(\"GPU is not available\")\n",
    "    device = 'cpu'  # 回退到CPU模式\n",
    "\n",
    "# 数据加载与清洗\n",
    "train=pd.read_csv('./train.csv').rename(columns={'temparature':'temperature'})  # 修正列名拼写错误\n",
    "test=pd.read_csv('./test.csv').rename(columns={'temparature':'temperature'})\n",
    "original=pd.read_csv(\"./Rainfall.csv\").rename(columns={'temparature':'temperature'})\n",
    "submission=pd.read_csv(\"./sample_submission.csv\")\n",
    "\n",
    "train.drop(columns=[\"id\"],inplace=True)  # 删除无关ID列\n",
    "test.drop(columns=[\"id\"],inplace=True)\n",
    "original['rainfall'] = original['rainfall'].map({'yes': 1, 'no': 0})  # 标签编码\n",
    "original.columns=[f.strip() for f in original.columns]  # 去除列名空格\n",
    "\n",
    "# 数据备份\n",
    "train_copy=train.copy()  # 创建原始数据副本\n",
    "test_copy=test.copy()\n",
    "original_copy=original.copy()\n",
    "\n",
    "# 数据源标记\n",
    "original[\"original\"]=1  # 标记原始数据集\n",
    "train[\"original\"]=0     # 标记比赛训练集\n",
    "test[\"original\"]=0      # 标记比赛测试集\n",
    "\n",
    "# 数据合并\n",
    "train=pd.concat([train,original],axis=0)  # 合并比赛数据与外部数据\n",
    "train.reset_index(inplace=True,drop=True)  # 重置索引\n",
    "\n",
    "# 目标变量设置\n",
    "target='rainfall'  # 定义预测目标列\n",
    "\n",
    "# 数据预览\n",
    "train.head()  # 展示处理后的数据前五行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T16:29:32.901931Z",
     "iopub.status.busy": "2025-03-10T16:29:32.901562Z",
     "iopub.status.idle": "2025-03-10T16:29:32.920605Z",
     "shell.execute_reply": "2025-03-10T16:29:32.919616Z",
     "shell.execute_reply.started": "2025-03-10T16:29:32.901905Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 创建PrettyTable对象用于美观地展示表格数据\n",
    "table = PrettyTable()\n",
    "# 定义表格列名：特征名、数据类型、训练集缺失率、测试集缺失率、原始数据缺失率、离散值比例\n",
    "table.field_names = ['Feature', 'Data Type', 'Train Missing %', 'Test Missing %', \"Original Missing%\", \"Discrete Ratio (Train)\"]\n",
    "\n",
    "# 遍历训练数据集的所有列（特征）\n",
    "for column in train_copy.columns:\n",
    "    # 获取当前列的数据类型（字符串格式）\n",
    "    data_type = str(train_copy[column].dtype)\n",
    "    \n",
    "    # 计算训练集缺失百分比 = (总行数 - 非空行数)/总行数 * 100，保留1位小数\n",
    "    non_null_count_train = np.round(100 - train_copy[column].count()/train_copy.shape[0]*100, 1)\n",
    "    \n",
    "    # 处理测试集缺失率（目标列没有测试集数据）\n",
    "    if column != target:\n",
    "        # 计算测试集缺失百分比（同上）\n",
    "        non_null_count_test = np.round(100 - test_copy[column].count()/test_copy.shape[0]*100, 1)\n",
    "    else:\n",
    "        non_null_count_test = \"NA\"  # 目标列在测试集中不存在\n",
    "    \n",
    "    # 计算原始数据集缺失百分比（同上）\n",
    "    non_null_count_orig = np.round(100 - original_copy[column].count()/original_copy.shape[0]*100, 1)\n",
    "    \n",
    "    # 计算离散值比例 = 唯一值数量 / 总样本数（保留4位小数）\n",
    "    # 用于判断特征是否是类别型或高基数特征\n",
    "    discrete_ratio = np.round(train_copy[column].nunique() / train_copy.shape[0], 4)\n",
    "    \n",
    "    # 将计算结果添加到表格行中\n",
    "    table.add_row([column, data_type, non_null_count_train, non_null_count_test, non_null_count_orig, discrete_ratio])\n",
    "\n",
    "# 打印完整的数据质量分析表格\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-03-10T16:29:32.923403Z",
     "iopub.status.busy": "2025-03-10T16:29:32.9231Z",
     "iopub.status.idle": "2025-03-10T16:29:33.51551Z",
     "shell.execute_reply": "2025-03-10T16:29:33.514307Z",
     "shell.execute_reply.started": "2025-03-10T16:29:32.923376Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_rainfall_distribution(data, title, ax, colors=None, shadow=True, startangle=90):\n",
    "    \"\"\"\n",
    "    生成增强型降雨分布饼图，支持自定义样式和布局\n",
    "    \n",
    "    参数说明：\n",
    "        data (DataFrame): 包含目标变量的数据集\n",
    "        title (str):     图表标题\n",
    "        ax (axes):      绘图坐标轴对象\n",
    "        colors (list):  自定义颜色列表（可选）\n",
    "        shadow (bool):  是否显示阴影（默认True）\n",
    "        startangle (int): 饼图起始角度（默认90度）\n",
    "    \"\"\"\n",
    "    # 统计目标变量分布\n",
    "    data_counts = data[target].value_counts().sort_index()\n",
    "    \n",
    "    # 定义分类标签（0:无雨，1:有雨）\n",
    "    labels = ['No Rain (0)', 'Rain (1)']  # 恢复英文标签\n",
    "    \n",
    "    # 获取对应分类的样本数量\n",
    "    sizes = [data_counts.get(0, 0), data_counts.get(1, 0)]\n",
    "    \n",
    "    # 设置默认颜色（蓝色表示无雨，红色表示有雨）\n",
    "    if colors is None:\n",
    "        colors = ['#3498db', '#e74c3c']\n",
    "    \n",
    "    # 设置饼图突出效果（突出显示有雨的部分）\n",
    "    explode = (0, 0.1)\n",
    "\n",
    "    # 绘制饼图核心参数\n",
    "    wedges, texts, autotexts = ax.pie(\n",
    "        sizes,\n",
    "        explode=explode, \n",
    "        labels=labels,\n",
    "        colors=colors,\n",
    "        autopct='%1.1f%%',\n",
    "        shadow=shadow,\n",
    "        startangle=startangle,\n",
    "        wedgeprops={'edgecolor': 'w', 'linewidth': 1}\n",
    "    )\n",
    "    \n",
    "    # 定制标签文本样式\n",
    "    for text in texts:\n",
    "        text.set_fontsize(12)\n",
    "        text.set_fontweight('bold')\n",
    "    \n",
    "    # 定制百分比文本样式\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_fontsize(11)\n",
    "        autotext.set_fontweight('bold')\n",
    "        autotext.set_color('white')\n",
    "    \n",
    "    # 添加数据集样本量标注\n",
    "    ax.text(\n",
    "        0, -1.2,\n",
    "        f\"Total samples: {len(data)}\",  \n",
    "        ha='center',\n",
    "        fontsize=10,\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", fc='#f0f0f0', ec='gray')\n",
    "    )\n",
    "    \n",
    "    # 确保饼图为正圆形\n",
    "    ax.axis('equal')\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "# 创建画布和子图（1行2列布局）\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# 定义色盲友好配色方案\n",
    "custom_colors = [\n",
    "    ['#3498db', '#e74c3c'],  # 蓝红组合（训练集）\n",
    "    ['#2ecc71', '#9b59b6']   # 绿紫组合（原始数据集）\n",
    "]\n",
    "\n",
    "# 绘制训练数据集分布\n",
    "plot_rainfall_distribution(train_copy, \"Training Dataset: Rainfall Distribution\", axes[0], colors=custom_colors[0])\n",
    "plot_rainfall_distribution(original, \"Original Dataset: Rainfall Distribution\", axes[1], colors=custom_colors[1])\n",
    "\n",
    "# 添加主标题\n",
    "fig.suptitle('Rainfall Distribution Comparison Across Datasets', fontsize=16, fontweight='bold', y=0.98)  \n",
    "\n",
    "# 添加图表脚注说明\n",
    "footnote = \"Note: Comparative visualization of class balance in the target variable across datasets.\"  \n",
    "plt.figtext(0.5, 0.01, footnote, ha='center', fontsize=10, style='italic')\n",
    "\n",
    "# 调整子图间距防止重叠\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-03-10T16:29:33.51722Z",
     "iopub.status.busy": "2025-03-10T16:29:33.516904Z",
     "iopub.status.idle": "2025-03-10T16:29:38.959207Z",
     "shell.execute_reply": "2025-03-10T16:29:38.957923Z",
     "shell.execute_reply.started": "2025-03-10T16:29:33.517194Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 定义专业配色方案（蓝色代表无雨，红色代表有雨）\n",
    "custom_palette = [\"#3498db\", \"#e74c3c\"]\n",
    "\n",
    "# 配置全局绘图样式\n",
    "plt.style.use('seaborn-v0_8-whitegrid')  # 使用带网格线的白底样式\n",
    "plt.rcParams['font.family'] = 'sans-serif'  # 设置通用字体类型\n",
    "plt.rcParams['font.sans-serif'] = ['Arial', 'Helvetica', 'DejaVu Sans']  # 备选无衬线字体\n",
    "\n",
    "# 获取连续型特征列表（排除对象类型且唯一值大于2的特征）\n",
    "cont_cols = [f for f in train.columns if train[f].dtype != 'O' and train[f].nunique()>2]\n",
    "n_rows = len(cont_cols)  # 确定需要绘制的行数（每个特征一行）\n",
    "\n",
    "# 创建自定义尺寸的画布（每行高度4.5英寸，DPI分辨率100）\n",
    "fig, axs = plt.subplots(n_rows, 2, figsize=(14, 4.5 * n_rows), dpi=100)\n",
    "\n",
    "# 确保axs始终为二维数组（处理单个特征的边界情况）\n",
    "if n_rows == 1:\n",
    "    axs = np.array([axs])\n",
    "\n",
    "# 设置画布背景颜色（浅灰色）\n",
    "fig.patch.set_facecolor('#f8f9fa')\n",
    "\n",
    "# 循环绘制每个连续特征的小提琴图\n",
    "for i, col in enumerate(cont_cols):\n",
    "    # ----------------- 训练集子图 -----------------\n",
    "    # 绘制左侧训练集分布图\n",
    "    sns.violinplot(\n",
    "        x=target, \n",
    "        y=col, \n",
    "        data=train_copy, \n",
    "        ax=axs[i, 0],\n",
    "        palette=custom_palette,\n",
    "        inner='quartile',  # 显示四分位线\n",
    "        linewidth=1.5,     # 调整线条粗细\n",
    "        cut=0              # 显示完整数据范围\n",
    "    )\n",
    "    # 设置训练集子图样式\n",
    "    axs[i, 0].set_title(f'{col.title()} Distribution by Rainfall (Train)', \n",
    "                       fontsize=14, fontweight='bold', pad=10)\n",
    "    axs[i, 0].set_xlabel('Rainfall', fontsize=12, labelpad=10)\n",
    "    axs[i, 0].set_ylabel(col.title(), fontsize=12, labelpad=10)\n",
    "    axs[i, 0].tick_params(labelsize=11)  # 调整刻度标签大小\n",
    "    axs[i, 0].grid(axis='y', linestyle='--', alpha=0.7)  # 添加横向虚线网格\n",
    "    \n",
    "    # ----------------- 原始数据集子图 -----------------\n",
    "    # 绘制右侧原始数据分布图\n",
    "    sns.violinplot(\n",
    "        x=target, \n",
    "        y=col, \n",
    "        data=original, \n",
    "        ax=axs[i, 1],\n",
    "        palette=custom_palette,\n",
    "        inner='quartile',\n",
    "        linewidth=1.5,\n",
    "        cut=0\n",
    "    )\n",
    "    # 设置原始数据集子图样式\n",
    "    axs[i, 1].set_title(f'{col.title()} Distribution by Rainfall (Original)', \n",
    "                       fontsize=14, fontweight='bold', pad=10)\n",
    "    axs[i, 1].set_xlabel('Rainfall', fontsize=12, labelpad=10)\n",
    "    axs[i, 1].set_ylabel(col.title(), fontsize=12, labelpad=10)\n",
    "    axs[i, 1].tick_params(labelsize=11)\n",
    "    axs[i, 1].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # ----------------- 通用设置 -----------------\n",
    "    # 统一设置x轴标签（两个子图）\n",
    "    for j in range(2):\n",
    "        axs[i, j].set_xticklabels(['No Rain (0)', 'Rain (1)'])  # 明确分类标签\n",
    "    \n",
    "    # 移除多余边框线（保留左侧和底部）\n",
    "    sns.despine(ax=axs[i, 0])\n",
    "    sns.despine(ax=axs[i, 1])\n",
    "\n",
    "# 添加全局标题（位于顶部0.99位置）\n",
    "fig.suptitle('Feature Distributions by Rainfall Status', \n",
    "            fontsize=16, fontweight='bold', y=0.99)\n",
    "\n",
    "# 优化布局参数\n",
    "fig.tight_layout(rect=[0, 0, 1, 0.97])  # 调整子图区域范围\n",
    "plt.subplots_adjust(hspace=0.35)       # 设置行间距为35%\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T16:29:38.960634Z",
     "iopub.status.busy": "2025-03-10T16:29:38.960328Z",
     "iopub.status.idle": "2025-03-10T16:29:38.970153Z",
     "shell.execute_reply": "2025-03-10T16:29:38.96893Z",
     "shell.execute_reply.started": "2025-03-10T16:29:38.960607Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def min_max_scaler(train, test, column):\n",
    "    '''\n",
    "    鲁棒型最小最大归一化方法（跨数据集）\n",
    "    \n",
    "    参数：\n",
    "        train (DataFrame): 训练数据集\n",
    "        test (DataFrame):  测试数据集\n",
    "        column (str):      需要归一化的特征名称\n",
    "    \n",
    "    特点：\n",
    "        - 跨数据集计算最大最小值，防止测试集出现越界值\n",
    "        - 避免传统方法中仅用训练集统计量导致的测试集溢出问题\n",
    "    '''\n",
    "    sc = MinMaxScaler()\n",
    "    \n",
    "    # 获取全局最大最小值（考虑训练集和测试集的分布）\n",
    "    max_val = max(train[column].max(), test[column].max())\n",
    "    min_val = min(train[column].min(), test[column].min())\n",
    "\n",
    "    # 执行归一化计算（公式：(x - min)/(max - min)）\n",
    "    train[column] = (train[column] - min_val) / (max_val - min_val)\n",
    "    test[column] = (test[column] - min_val) / (max_val - min_val)\n",
    "    \n",
    "    return train, test  \n",
    "\n",
    "def OHE(train_df, test_df, cols, target):\n",
    "    '''\n",
    "    增强型独热编码方法（带冗余类别处理）\n",
    "    \n",
    "    参数：\n",
    "        train_df (DataFrame): 训练数据\n",
    "        test_df (DataFrame):  测试数据\n",
    "        cols (list):         需要编码的分类特征列表\n",
    "        target (str):        目标变量名称\n",
    "    \n",
    "    特点：\n",
    "        - 合并数据集进行编码，避免类别缺失问题\n",
    "        - 自动删除频率最低的类别（减少维度+避免过拟合）\n",
    "        - 防止测试集出现训练集未包含的类别\n",
    "    '''\n",
    "    # 合并数据集（确保编码完整性）\n",
    "    combined = pd.concat([train_df, test_df], axis=0)\n",
    "    \n",
    "    for col in cols:\n",
    "        # 生成独热编码矩阵\n",
    "        one_hot = pd.get_dummies(combined[col])\n",
    "        \n",
    "        # 识别并删除最少出现的类别（基于频次统计）\n",
    "        counts = combined[col].value_counts()\n",
    "        min_count_category = counts.idxmin()\n",
    "        one_hot = one_hot.drop(min_count_category, axis=1)\n",
    "        \n",
    "        # 重命名列（添加特征后缀避免冲突）\n",
    "        one_hot.columns = [str(f) + col + \"_OHE\" for f in one_hot.columns]\n",
    "        \n",
    "        # 合并编码结果并去重\n",
    "        combined = pd.concat([combined, one_hot], axis=\"columns\")\n",
    "        combined = combined.loc[:, ~combined.columns.duplicated()]\n",
    "    \n",
    "    # 数据集拆分与后处理\n",
    "    train_ohe = combined[:len(train_df)]          # 还原训练集\n",
    "    test_ohe = combined[len(train_df):]           # 提取测试集\n",
    "    test_ohe.reset_index(inplace=True, drop=True) # 重置索引\n",
    "    \n",
    "    # 移除测试集中的目标变量（如果存在）\n",
    "    if target in test_ohe.columns:\n",
    "        test_ohe.drop(columns=[target], inplace=True)\n",
    "        \n",
    "    return train_ohe, test_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T16:29:38.971617Z",
     "iopub.status.busy": "2025-03-10T16:29:38.971321Z",
     "iopub.status.idle": "2025-03-10T16:29:39.055838Z",
     "shell.execute_reply": "2025-03-10T16:29:39.054833Z",
     "shell.execute_reply.started": "2025-03-10T16:29:38.97159Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def handle_missing_values(train_df, test_df, target=\"rainfall\", n_components=1):\n",
    "    \"\"\"\n",
    "    处理包含缺失值的数值型数据集，主要步骤包括：\n",
    "    1. 使用中位数进行缺失值填补\n",
    "    2. 创建缺失值指示器\n",
    "    3. 应用SVD降维合并指示器列\n",
    "    \n",
    "    参数：\n",
    "    -----------\n",
    "    train_df : pandas DataFrame\n",
    "        包含缺失值的训练数据集（仅数值型）\n",
    "    test_df : pandas DataFrame\n",
    "        包含缺失值的测试数据集（仅数值型）\n",
    "    target : str, default=\"rainfall\"\n",
    "        目标变量名称（将排除在填补和处理之外）\n",
    "    n_components : int, default=1\n",
    "        SVD后保留的主成分数量\n",
    "        \n",
    "    返回：\n",
    "    --------\n",
    "    train_processed : pandas DataFrame\n",
    "        处理后的训练数据，包含填补值和SVD特征\n",
    "    test_processed : pandas DataFrame\n",
    "        处理后的测试数据，包含填补值和SVD特征\n",
    "    \"\"\"\n",
    "    # 创建副本以避免修改原始数据集\n",
    "    train_processed = train_df.copy()\n",
    "    test_processed = test_df.copy()\n",
    "    \n",
    "    # 处理目标列\n",
    "    y_train = None\n",
    "    if target in train_df.columns:\n",
    "        y_train = train_processed[target].copy()\n",
    "        train_processed = train_processed.drop(columns=[target])\n",
    "    \n",
    "    # 获取特征列（排除目标变量）\n",
    "    train_features = train_processed.columns.tolist()\n",
    "    \n",
    "    # 获取训练集和测试集的共同特征\n",
    "    common_features = [col for col in train_features if col in test_df.columns]\n",
    "    \n",
    "    # 仅使用共同特征进行填补以确保一致性\n",
    "    train_subset = train_processed[common_features]\n",
    "    test_subset = test_processed[common_features]\n",
    "    \n",
    "    # 步骤1：填补 - 创建数值型列的简单填补器\n",
    "    numeric_imputer = SimpleImputer(strategy='median')\n",
    "    \n",
    "    # 仅在共同特征上拟合填补器\n",
    "    numeric_imputer.fit(train_subset)\n",
    "    \n",
    "    # 转换两个数据集\n",
    "    train_imputed_values = numeric_imputer.transform(train_subset)\n",
    "    test_imputed_values = numeric_imputer.transform(test_subset)\n",
    "    \n",
    "    # 从填补值创建DataFrame\n",
    "    train_imputed = pd.DataFrame(\n",
    "        train_imputed_values,\n",
    "        columns=common_features,\n",
    "        index=train_processed.index\n",
    "    )\n",
    "    \n",
    "    test_imputed = pd.DataFrame(\n",
    "        test_imputed_values,\n",
    "        columns=common_features,\n",
    "        index=test_processed.index\n",
    "    )\n",
    "    \n",
    "    # 将非共同特征添加回训练集（不进行填补）\n",
    "    non_common_features = [col for col in train_features if col not in common_features]\n",
    "    for col in non_common_features:\n",
    "        train_imputed[col] = train_processed[col]\n",
    "    \n",
    "    # 步骤2：为共同特征创建缺失值指示器\n",
    "    indicator_cols = []\n",
    "    for col in common_features:\n",
    "        indicator_name = f'{col}_missing'\n",
    "        train_imputed[indicator_name] = train_df[col].isna().astype(int)\n",
    "        test_imputed[indicator_name] = test_df[col].isna().astype(int)\n",
    "        indicator_cols.append(indicator_name)\n",
    "    \n",
    "    # 步骤3：应用SVD将指示器列合并到更少的维度\n",
    "    if indicator_cols and len(indicator_cols) > 1:  # 仅在存在多个指示器时应用SVD\n",
    "        # 使用指定数量的主成分初始化SVD\n",
    "        svd = TruncatedSVD(n_components=min(n_components, len(indicator_cols)))\n",
    "        \n",
    "        # 在训练数据指示器上拟合SVD并转换两个数据集\n",
    "        missing_indicators_train = train_imputed[indicator_cols].values\n",
    "        missing_indicators_test = test_imputed[indicator_cols].values\n",
    "        \n",
    "        # 仅在存在缺失值时进行SVD\n",
    "        if np.any(missing_indicators_train):\n",
    "            # 拟合和转换\n",
    "            missing_svd_train = svd.fit_transform(missing_indicators_train)\n",
    "            missing_svd_test = svd.transform(missing_indicators_test)\n",
    "            \n",
    "            # 将SVD组件添加到数据集中\n",
    "            for i in range(n_components):\n",
    "                train_imputed[f'missing_svd_{i}'] = missing_svd_train[:, i]\n",
    "                test_imputed[f'missing_svd_{i}'] = missing_svd_test[:, i]\n",
    "            \n",
    "            # 可选地删除原始指示器列（如果不再需要）\n",
    "            train_imputed.drop(columns=indicator_cols, inplace=True)\n",
    "            test_imputed.drop(columns=indicator_cols, inplace=True)\n",
    "    \n",
    "    # 如果存在目标列，将其添加回训练数据\n",
    "    if y_train is not None:\n",
    "        train_imputed[target] = y_train\n",
    "    \n",
    "    return train_imputed, test_imputed\n",
    "    \n",
    "train_imputed, test_imputed = handle_missing_values(train, test, n_components=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T16:29:39.067871Z",
     "iopub.status.busy": "2025-03-10T16:29:39.067484Z",
     "iopub.status.idle": "2025-03-10T16:29:39.170993Z",
     "shell.execute_reply": "2025-03-10T16:29:39.169576Z",
     "shell.execute_reply.started": "2025-03-10T16:29:39.067835Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    气象特征工程函数（防数据泄漏版）\n",
    "    \n",
    "    基于气象学原理和数据分析创建新特征，主要特点：\n",
    "    - 使用'day'表示一年中的第几天（1-365）\n",
    "    - 所有特征仅使用当日及历史数据生成\n",
    "    - 严格避免使用目标变量（rainfall）\n",
    "    \n",
    "    参数：\n",
    "        df (DataFrame): 原始气象数据（需包含基础气象指标）\n",
    "    \n",
    "    返回：\n",
    "        enhanced_df (DataFrame): 包含42+新特征的数据框\n",
    "    \"\"\"\n",
    "    # 创建副本避免污染原始数据\n",
    "    enhanced_df = df.copy()\n",
    "    \n",
    "    # ================= 基础衍生特征 =================\n",
    "    # 1. 温度范围（昼夜温差）\n",
    "    enhanced_df['temp_range'] = enhanced_df['maxtemp'] - enhanced_df['mintemp']\n",
    "    \n",
    "    # 2. 露点差（气温与露点温差，反映空气饱和程度）\n",
    "    enhanced_df['dewpoint_depression'] = enhanced_df['temperature'] - enhanced_df['dewpoint']\n",
    "    \n",
    "    # 3. 气压日变化（与前日差值，反映气压系统移动）\n",
    "    enhanced_df['pressure_change'] = enhanced_df['pressure'].diff().fillna(0)\n",
    "    \n",
    "    # ================= 比值类特征 =================\n",
    "    # 4. 湿度露点比（clip防止除以0）\n",
    "    enhanced_df['humidity_dewpoint_ratio'] = enhanced_df['humidity'] / enhanced_df['dewpoint'].clip(lower=0.1)\n",
    "    \n",
    "    # 5. 云量日照比（反映云层对日照的遮挡效应）\n",
    "    enhanced_df['cloud_sunshine_ratio'] = enhanced_df['cloud'] / enhanced_df['sunshine'].clip(lower=0.1)\n",
    "    \n",
    "    # 6. 风-湿度综合因子（风速与相对湿度的相互作用）\n",
    "    enhanced_df['wind_humidity_factor'] = enhanced_df['windspeed'] * (enhanced_df['humidity'] / 100)\n",
    "    \n",
    "    # ================= 复合气象指数 =================\n",
    "    # 7. 温湿指数（简化版体感温度公式）\n",
    "    enhanced_df['temp_humidity_index'] = (0.8 * enhanced_df['temperature']) + \\\n",
    "                                        ((enhanced_df['humidity'] / 100) * \\\n",
    "                                        (enhanced_df['temperature'] - 14.3)) + 46.4\n",
    "    \n",
    "    # 8. 气压变化加速度（二阶差分）\n",
    "    enhanced_df['pressure_acceleration'] = enhanced_df['pressure_change'].diff().fillna(0)\n",
    "    \n",
    "    # ================= 时间维度特征 =================\n",
    "    # 9. 月份特征（每30天为一个月，上限12月）\n",
    "    enhanced_df['month'] = ((enhanced_df['day'] - 1) // 30) + 1\n",
    "    enhanced_df['month'] = enhanced_df['month'].clip(upper=12)\n",
    "    \n",
    "    # 10. 季节特征（每3个月为一个季节）\n",
    "    enhanced_df['season'] = ((enhanced_df['month'] - 1) // 3) + 1\n",
    "    \n",
    "    # 11. 日期周期性编码（捕捉年周期规律）\n",
    "    enhanced_df['day_of_year_sin'] = np.sin(2 * np.pi * enhanced_df['day'] / 365)\n",
    "    enhanced_df['day_of_year_cos'] = np.cos(2 * np.pi * enhanced_df['day'] / 365)\n",
    "    \n",
    "    # ================= 滑动窗口特征 =================\n",
    "    # 12. 关键指标滑动平均（3/7/14天窗口）\n",
    "    for window in [3, 7, 14]:\n",
    "        # 温度滑动平均（至少需要1个观测值）\n",
    "        enhanced_df[f'temperature_rolling_{window}d'] = enhanced_df['temperature'].rolling(window=window, min_periods=1).mean()\n",
    "        # 气压滑动平均\n",
    "        enhanced_df[f'pressure_rolling_{window}d'] = enhanced_df['pressure'].rolling(window=window, min_periods=1).mean()\n",
    "        # 湿度滑动平均\n",
    "        enhanced_df[f'humidity_rolling_{window}d'] = enhanced_df['humidity'].rolling(window=window, min_periods=1).mean()\n",
    "        # 云量滑动平均\n",
    "        enhanced_df[f'cloud_rolling_{window}d'] = enhanced_df['cloud'].rolling(window=window, min_periods=1).mean()\n",
    "        # 风速滑动平均\n",
    "        enhanced_df[f'windspeed_rolling_{window}d'] = enhanced_df['windspeed'].rolling(window=window, min_periods=1).mean()\n",
    "    \n",
    "    # ================= 趋势特征 =================\n",
    "    # 13. 三日趋势（当前值与3天前差值）\n",
    "    enhanced_df['temp_trend_3d'] = enhanced_df['temperature'].diff(3).fillna(0)  # 温度趋势\n",
    "    enhanced_df['pressure_trend_3d'] = enhanced_df['pressure'].diff(3).fillna(0)  # 气压趋势\n",
    "    enhanced_df['humidity_trend_3d'] = enhanced_df['humidity'].diff(3).fillna(0)  # 湿度趋势\n",
    "    \n",
    "    # ================= 极端事件指标 =================\n",
    "    # 14. 温度极端值（超出5%和95%分位数）\n",
    "    enhanced_df['extreme_temp'] = (enhanced_df['temperature'] > enhanced_df['temperature'].quantile(0.95)) | \\\n",
    "                                 (enhanced_df['temperature'] < enhanced_df['temperature'].quantile(0.05))\n",
    "    enhanced_df['extreme_temp'] = enhanced_df['extreme_temp'].astype(int)\n",
    "    \n",
    "    # 湿度极端值\n",
    "    enhanced_df['extreme_humidity'] = (enhanced_df['humidity'] > enhanced_df['humidity'].quantile(0.95)) | \\\n",
    "                                     (enhanced_df['humidity'] < enhanced_df['humidity'].quantile(0.05))\n",
    "    enhanced_df['extreme_humidity'] = enhanced_df['extreme_humidity'].astype(int)\n",
    "    \n",
    "    # 气压极端值 \n",
    "    enhanced_df['extreme_pressure'] = (enhanced_df['pressure'] > enhanced_df['pressure'].quantile(0.95)) | \\\n",
    "                                     (enhanced_df['pressure'] < enhanced_df['pressure'].quantile(0.05))\n",
    "    enhanced_df['extreme_pressure'] = enhanced_df['extreme_pressure'].astype(int)\n",
    "    \n",
    "    # ================= 交互特征 =================\n",
    "    # 15. 关键变量交互项（捕捉非线性关系）\n",
    "    enhanced_df['temp_humidity_interaction'] = enhanced_df['temperature'] * enhanced_df['humidity']  # 温湿交互\n",
    "    enhanced_df['pressure_wind_interaction'] = enhanced_df['pressure'] * enhanced_df['windspeed']  # 气压风速交互\n",
    "    enhanced_df['cloud_sunshine_interaction'] = enhanced_df['cloud'] * enhanced_df['sunshine']  # 云量日照交互\n",
    "    enhanced_df['dewpoint_humidity_interaction'] = enhanced_df['dewpoint'] * enhanced_df['humidity']  # 露点湿度交互\n",
    "    \n",
    "    # ================= 波动性特征 =================\n",
    "    # 16. 滑动标准差（7/14天窗口，至少4个观测值）\n",
    "    for window in [7, 14]:\n",
    "        # 温度波动性\n",
    "        enhanced_df[f'temp_std_{window}d'] = enhanced_df['temperature'].rolling(window=window, min_periods=4).std().fillna(0)\n",
    "        # 气压波动性\n",
    "        enhanced_df[f'pressure_std_{window}d'] = enhanced_df['pressure'].rolling(window=window, min_periods=4).std().fillna(0)\n",
    "        # 湿度波动性\n",
    "        enhanced_df[f'humidity_std_{window}d'] = enhanced_df['humidity'].rolling(window=window, min_periods=4).std().fillna(0)\n",
    "    \n",
    "    return enhanced_df\n",
    "    \n",
    "# 应用特征工程（训练集和测试集独立处理，避免数据泄漏）\n",
    "train_fe = engineer_features(train_imputed)\n",
    "test_fe = engineer_features(test_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T16:29:39.172508Z",
     "iopub.status.busy": "2025-03-10T16:29:39.172182Z",
     "iopub.status.idle": "2025-03-10T16:29:39.2359Z",
     "shell.execute_reply": "2025-03-10T16:29:39.234649Z",
     "shell.execute_reply.started": "2025-03-10T16:29:39.172467Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===================== 特征选择与预处理 ===================== #\n",
    "# 生成最终特征列表（排除目标变量）\n",
    "final_features = [f for f in train_fe.columns if f not in [target]]\n",
    "# 使用集合去重（防止特征重复）\n",
    "final_features = [*set(final_features)]\n",
    "\n",
    "# 初始化标准化器（Z-score标准化）\n",
    "# 使用StandardScaler保证训练集/测试集使用相同的缩放参数\n",
    "sc = StandardScaler()\n",
    "\n",
    "# 创建数据副本以避免污染原始特征工程数据\n",
    "train_scaled = train_fe.copy()  # 训练集副本\n",
    "test_scaled = test_fe.copy()    # 测试集副本\n",
    "\n",
    "# 对数值型特征进行标准化处理\n",
    "# 训练集：拟合缩放器并转换数据（fit_transform）\n",
    "# 测试集：仅使用训练集的参数进行转换（transform）\n",
    "train_scaled[final_features] = sc.fit_transform(train_fe[final_features])\n",
    "test_scaled[final_features] = sc.transform(test_fe[final_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T16:29:39.239745Z",
     "iopub.status.busy": "2025-03-10T16:29:39.239428Z",
     "iopub.status.idle": "2025-03-10T16:29:39.799485Z",
     "shell.execute_reply": "2025-03-10T16:29:39.798338Z",
     "shell.execute_reply.started": "2025-03-10T16:29:39.23972Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def post_processor(train, test):\n",
    "    \"\"\"\n",
    "    后处理函数：检测并删除高度相关的重复特征\n",
    "    \n",
    "    功能：\n",
    "    1. 识别训练集中完全相同的特征列\n",
    "    2. 在训练集和测试集中同步删除重复特征\n",
    "    3. 维护特征一致性，避免数据泄露\n",
    "    \n",
    "    参数：\n",
    "        train (DataFrame): 处理后的训练数据集\n",
    "        test (DataFrame): 处理后的测试数据集\n",
    "    \n",
    "    返回：\n",
    "        train_cop, test_cop: 删除重复列后的数据集副本\n",
    "    \"\"\"\n",
    "    # 获取除目标变量外的所有特征列\n",
    "    cols = train.drop(columns=[target]).columns\n",
    "    \n",
    "    # 创建数据副本（避免修改原始数据）\n",
    "    train_cop = train.copy()  # 训练集副本\n",
    "    test_cop = test.copy()    # 测试集副本\n",
    "    \n",
    "    # 初始化待删除列列表\n",
    "    drop_cols = []\n",
    "    \n",
    "    # 双重循环遍历所有特征组合（O(n²)时间复杂度）\n",
    "    for i, feature in enumerate(cols):\n",
    "        # 比较当前特征与后续所有特征\n",
    "        for j in range(i+1, len(cols)):\n",
    "            # 计算两列绝对差异之和（若为0表示完全相同）\n",
    "            if sum(abs(train_cop[feature] - train_cop[cols[j]])) == 0:\n",
    "                # 将重复列加入删除列表（保留先出现的特征）\n",
    "                if cols[j] not in drop_cols:\n",
    "                    drop_cols.append(cols[j])\n",
    "    \n",
    "    print(\"重复特征列：\", drop_cols)  # 打印被删除的特征\n",
    "    \n",
    "    # 同步删除训练集和测试集中的重复列\n",
    "    train_cop.drop(columns=drop_cols, inplace=True)  # 就地修改训练集副本\n",
    "    test_cop.drop(columns=drop_cols, inplace=True)  # 测试集保持相同操作\n",
    "    \n",
    "    return train_cop, test_cop\n",
    "\n",
    "# ================ 函数调用与数据分割 ================ #\n",
    "# 应用后处理函数（输入标准化后的数据）\n",
    "train_cop, test_cop = post_processor(train_scaled, test_scaled)\n",
    "\n",
    "# 特征矩阵与目标变量分离\n",
    "X_train = train_cop.drop(columns=[target])  # 训练特征矩阵\n",
    "y_train = train[target]                    # 训练目标变量（使用原始数据的目标列）\n",
    "\n",
    "X_test = test_cop.copy()  # 测试集特征矩阵（不含目标变量）\n",
    "\n",
    "# 输出处理后的数据维度\n",
    "print(\"最终维度：\", X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T16:29:39.801434Z",
     "iopub.status.busy": "2025-03-10T16:29:39.801161Z",
     "iopub.status.idle": "2025-03-10T16:29:39.815124Z",
     "shell.execute_reply": "2025-03-10T16:29:39.813735Z",
     "shell.execute_reply.started": "2025-03-10T16:29:39.801409Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_most_important_features(X_train, y_train, n, model_input):\n",
    "    \"\"\"\n",
    "    交叉验证特征重要性分析函数\n",
    "    \n",
    "    功能：\n",
    "    1. 根据输入模型类型自动配置参数（XGBoost/LightGBM/CatBoost）\n",
    "    2. 5折交叉验证训练模型并收集特征重要性\n",
    "    3. 可视化Top10特征重要性并显示平均AUC\n",
    "    4. 返回前N个重要特征列表\n",
    "    \n",
    "    参数：\n",
    "        X_train (DataFrame): 训练特征矩阵\n",
    "        y_train (Series): 训练目标变量\n",
    "        n (int): 需要返回的重要特征数量\n",
    "        model_input (str): 模型标识符（包含'xgb'/'cat'则使用对应模型，默认LightGBM）\n",
    "    \n",
    "    返回：\n",
    "        top_n_features (list): 前N个重要特征名称列表\n",
    "    \"\"\"\n",
    "    # ================= 模型参数配置 =================\n",
    "    # XGBoost参数配置（根据设备类型自动启用GPU加速）\n",
    "    xgb_params = {\n",
    "        'n_jobs': -1,  # 使用全部CPU核心\n",
    "        'eval_metric': 'logloss',  # 评估指标\n",
    "        'objective': 'binary:logistic',  # 二分类任务\n",
    "        'tree_method': 'hist',  # 默认使用直方图算法\n",
    "        'verbosity': 0,  # 关闭训练日志\n",
    "        'random_state': 42,  # 随机种子\n",
    "    }\n",
    "    if device == 'gpu':\n",
    "        xgb_params.update({\n",
    "            'tree_method': 'gpu_hist',  # GPU加速的直方图算法\n",
    "            'predictor': 'gpu_predictor'  # GPU预测\n",
    "        })\n",
    "\n",
    "    # LightGBM参数配置\n",
    "    lgb_params = {\n",
    "        'objective': 'binary',  # 二分类任务\n",
    "        'metric': 'logloss',  # 评估指标\n",
    "        'boosting_type': 'gbdt',  # 梯度提升树\n",
    "        'random_state': 42,\n",
    "        'device': device,  # 自动选择计算设备\n",
    "        'verbose': -1  # 完全关闭训练输出\n",
    "    }\n",
    "\n",
    "    # CatBoost参数配置\n",
    "    cb_params = {\n",
    "        'grow_policy': 'Depthwise',  # 深度优先生长策略\n",
    "        'bootstrap_type': 'Bayesian',  # 贝叶斯自助采样\n",
    "        'od_type': 'Iter',  # 过拟合检测类型\n",
    "        'eval_metric': 'AUC',  # 评估指标\n",
    "        'loss_function': 'Logloss',  # 损失函数\n",
    "        'random_state': 42,\n",
    "        'task_type': device.upper(),  # 设备类型大写转换\n",
    "    }\n",
    "\n",
    "    # ================= 模型选择器 =================\n",
    "    if 'xgb' in model_input:\n",
    "        model = xgb.XGBClassifier(**xgb_params)\n",
    "    elif 'cat' in model_input:\n",
    "        model = CatBoostClassifier(**cb_params)\n",
    "    else:\n",
    "        model = lgb.LGBMClassifier(**lgb_params)\n",
    "\n",
    "    # ================= 交叉验证框架 =================\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    auc_scores = []  # 存储各fold的AUC得分\n",
    "    feature_importances_list = []  # 存储各fold的特征重要性\n",
    "\n",
    "    # ================= 交叉验证循环 =================\n",
    "    for train_idx, val_idx in kfold.split(X_train):\n",
    "        # 数据划分（保持原始索引）\n",
    "        X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        # 模型训练（LightGBM需要保留进度条输出）\n",
    "        if \"lgb\" not in model_input:\n",
    "            model.fit(X_train_fold, y_train_fold, verbose=False)  # 非LGBM模型关闭训练日志\n",
    "        else:\n",
    "            model.fit(X_train_fold, y_train_fold)  # LGBM使用内置进度条\n",
    "\n",
    "        # 预测与评估\n",
    "        y_pred = model.predict_proba(X_val_fold)[:, 1]\n",
    "        auc_scores.append(roc_auc_score(y_val_fold, y_pred))\n",
    "        \n",
    "        # 收集特征重要性（不同模型的特征重要性自动对齐）\n",
    "        feature_importances_list.append(model.feature_importances_)\n",
    "\n",
    "    # ================= 结果聚合 =================\n",
    "    avg_auc = np.mean(auc_scores)  # 平均AUC得分\n",
    "    avg_feature_importances = np.mean(feature_importances_list, axis=0)  # 平均特征重要性\n",
    "\n",
    "    # ================= 特征排序 =================\n",
    "    # 创建（特征名，重要性）元组列表\n",
    "    feature_importance_list = [(X_train.columns[i], imp) \n",
    "                              for i, imp in enumerate(avg_feature_importances)]\n",
    "    # 按重要性降序排序\n",
    "    sorted_features = sorted(feature_importance_list, \n",
    "                            key=lambda x: x[1], \n",
    "                            reverse=True)\n",
    "    # 提取前N个特征名称\n",
    "    top_n_features = [feature[0] for feature in sorted_features[:n]]\n",
    "\n",
    "    # ================= 可视化模块 =================\n",
    "    display_features = top_n_features[:10]  # 仅展示前10个特征\n",
    "    importance_values = [avg_feature_importances[X_train.columns.get_loc(f)] for f in display_features]\n",
    "\n",
    "    # 创建颜色渐变（从浅红到深红）\n",
    "    colors = plt.cm.YlOrRd(np.linspace(1, 0.3, len(display_features)))\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    bars = plt.barh(range(len(display_features)), \n",
    "                importance_values, \n",
    "                color=colors,\n",
    "                edgecolor='black',\n",
    "                alpha=0.8)\n",
    "\n",
    "    # 图表装饰\n",
    "    plt.yticks(range(len(display_features)), display_features, fontsize=14)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.xlabel('Average Feature Importance', fontsize=14, labelpad=15)\n",
    "    plt.ylabel('Features', fontsize=14, labelpad=15)\n",
    "    plt.title(f'Top {10} of {n} Feature Importances with ROC AUC score {avg_auc:.4f}', fontsize=16, pad=20, fontweight='bold')\n",
    "    plt.gca().invert_yaxis()  # 倒置Y轴使最重要特征在上方\n",
    "    # 添加网格线\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    # 添加数据标签\n",
    "    for idx, value in enumerate([avg_feature_importances[X_train.columns.get_loc(f)] \n",
    "                                for f in display_features]):\n",
    "        plt.text(value + 0.005, idx, f'{value:.3f}', fontsize=12, va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return top_n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T16:29:39.816804Z",
     "iopub.status.busy": "2025-03-10T16:29:39.816403Z",
     "iopub.status.idle": "2025-03-10T16:31:24.083504Z",
     "shell.execute_reply": "2025-03-10T16:31:24.082324Z",
     "shell.execute_reply.started": "2025-03-10T16:29:39.816747Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "n_imp_features_cat=get_most_important_features(X_train.reset_index(drop=True), y_train,50, 'cat')\n",
    "n_imp_features_xgb=get_most_important_features(X_train.reset_index(drop=True), y_train,50, 'xgb')\n",
    "n_imp_features_lgbm=get_most_important_features(X_train.reset_index(drop=True), y_train,50, 'lgbm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T16:31:24.084882Z",
     "iopub.status.busy": "2025-03-10T16:31:24.084575Z",
     "iopub.status.idle": "2025-03-10T16:31:24.093643Z",
     "shell.execute_reply": "2025-03-10T16:31:24.092723Z",
     "shell.execute_reply.started": "2025-03-10T16:31:24.084856Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ================== 多模型特征集成与数据重构 ================== #\n",
    "# 集成三个模型的特征重要性结果（取并集）\n",
    "n_imp_features = [*set(n_imp_features_xgb + n_imp_features_lgbm + n_imp_features_cat)]\n",
    "print(f\"通过三种算法共筛选出 {len(n_imp_features)} 个重要特征进入最终模型\")\n",
    "\n",
    "# 重构训练集和测试集特征矩阵（仅保留重要特征）\n",
    "X_train = X_train[n_imp_features]  # 训练集特征筛选\n",
    "X_test = X_test[n_imp_features]    # 测试集保持相同特征子集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T16:31:24.095457Z",
     "iopub.status.busy": "2025-03-10T16:31:24.095025Z",
     "iopub.status.idle": "2025-03-10T16:31:24.117452Z",
     "shell.execute_reply": "2025-03-10T16:31:24.116238Z",
     "shell.execute_reply.started": "2025-03-10T16:31:24.095416Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ================== 类别权重计算模块 ================== #\n",
    "# 获取训练集目标变量的唯一类别（适用于多分类场景）\n",
    "classes = np.unique(y_train)  # 自动识别所有唯一类别标签\n",
    "\n",
    "# 构建类别到索引的映射字典（处理非数值型标签）\n",
    "class_to_index = {cls: idx for idx, cls in enumerate(classes)}  # 示例输出：{0:0, 1:1}\n",
    "\n",
    "# 将目标变量转换为数值索引（适配np.bincount的输入要求）\n",
    "y_train_numeric = np.array([class_to_index[cls] for cls in y_train])  # 将标签映射为连续整数\n",
    "\n",
    "# 统计每个类别的样本数量（返回数组索引对应类别，值对应样本数）\n",
    "class_counts = np.bincount(y_train_numeric)  # 示例输出：[7000, 1567]\n",
    "\n",
    "# 计算总样本量（用于后续权重计算）\n",
    "total_samples = len(y_train_numeric)  # 示例输出：8567\n",
    "\n",
    "# 计算平衡类别权重（Scikit-learn标准算法）\n",
    "# 公式：总样本数 / (类别数 * 各类别样本数)\n",
    "class_weights = total_samples / (len(classes) * class_counts)  # 示例输出：[0.61, 2.73]\n",
    "\n",
    "# 构建类别权重字典（适配大多数机器学习框架的输入格式）\n",
    "class_weights_dict = {cls: weight for cls, weight in zip(classes, class_weights)}  # 示例输出：{0:0.61, 1:2.73}\n",
    "\n",
    "# 打印诊断信息（帮助确认数据分布）\n",
    "print(\"Class counts:\", class_counts)       # 类别样本分布\n",
    "print(\"Total samples:\", total_samples)     # 总训练样本量\n",
    "print(\"Class weights:\", class_weights)     # 计算后的权重数组\n",
    "print(\"Class weights dictionary:\", class_weights_dict)  # 适配模型的字典格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T16:31:24.119311Z",
     "iopub.status.busy": "2025-03-10T16:31:24.118881Z",
     "iopub.status.idle": "2025-03-10T16:31:24.136145Z",
     "shell.execute_reply": "2025-03-10T16:31:24.134927Z",
     "shell.execute_reply.started": "2025-03-10T16:31:24.119279Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def optimizer():\n",
    "    '''优化器工厂函数，提供四种优化器配置'''\n",
    "    # SGD优化器（带Nesterov动量加速）\n",
    "    sgd = tensorflow.keras.optimizers.SGD(\n",
    "        learning_rate=0.005,  # 基础学习率\n",
    "        momentum=0.5,         # 动量系数\n",
    "        nesterov=True         # 启用Nesterov动量\n",
    "    )\n",
    "    # RMSprop优化器（适合非平稳目标）\n",
    "    rms = tensorflow.keras.optimizers.RMSprop(\n",
    "        rho=0.9               # 梯度平方的移动平均系数\n",
    "    )\n",
    "    # Nadam优化器（Adam+Nesterov）\n",
    "    nadam = tensorflow.keras.optimizers.Nadam(\n",
    "        learning_rate=0.001,  # 较低的基础学习率\n",
    "        beta_1=0.9,           # 一阶矩估计衰减率\n",
    "        beta_2=0.999,         # 二阶矩估计衰减率\n",
    "        epsilon=1e-07         # 数值稳定项\n",
    "    )\n",
    "    # 标准Adam优化器\n",
    "    adam = tensorflow.keras.optimizers.Adam(\n",
    "        amsgrad=True          # 启用AMSGrad变体\n",
    "    )\n",
    "    return sgd, rms, nadam, adam\n",
    "\n",
    "def init_ann1(num_classes, input_dim):\n",
    "    '''\n",
    "    构建浅层神经网络（适用于小规模数据集）\n",
    "    网络架构：16-4-输出层\n",
    "    特点：低容量模型，适合简单模式学习\n",
    "    '''\n",
    "    sgd, rms, nadam, adam = optimizer()\n",
    "    \n",
    "    ann = Sequential(name=\"Shallow_Net\")\n",
    "    # 输入层（He初始化适合ReLU激活）\n",
    "    ann.add(Dense(16, input_dim=input_dim, \n",
    "                kernel_initializer='he_uniform', \n",
    "                activation='relu'))\n",
    "    # 轻度Dropout防止过拟合\n",
    "    ann.add(Dropout(0.1))  # 保留90%的神经元\n",
    "    \n",
    "    # 隐藏层（瓶颈设计压缩特征）\n",
    "    ann.add(Dense(4, kernel_initializer='he_uniform', \n",
    "                activation='relu'))  # 无Dropout保持特征完整性\n",
    "    \n",
    "    # 输出层（多分类使用softmax）\n",
    "    ann.add(Dense(num_classes, activation='softmax',\n",
    "                kernel_initializer='he_uniform'))\n",
    "    \n",
    "    # 编译模型（注意：binary_crossentropy应与sigmoid配合使用）\n",
    "    ann.compile(loss='binary_crossentropy',  # 二分类交叉熵\n",
    "               optimizer=sgd,               # 使用带动量的SGD\n",
    "               metrics=['accuracy', tensorflow.keras.metrics.AUC(name='auc')])\n",
    "    return ann\n",
    "\n",
    "def init_ann2(num_classes, input_dim):  \n",
    "    '''\n",
    "    构建深层神经网络（适用于复杂模式提取）\n",
    "    网络架构：128-4-输出层\n",
    "    特点：高容量模型，带深度正则化\n",
    "    '''\n",
    "    sgd, rms, nadam, adam = optimizer()\n",
    "    \n",
    "    ann2 = Sequential(name=\"Deep_Net\")\n",
    "    # 宽输入层捕获复杂特征\n",
    "    ann2.add(Dense(128, input_dim=input_dim,\n",
    "                 kernel_initializer='he_uniform',\n",
    "                 activation='relu'))\n",
    "    ann2.add(Dropout(0.3))  # 强Dropout防止过拟合\n",
    "    \n",
    "    # 特征压缩层\n",
    "    ann2.add(Dense(4, activation='relu',\n",
    "                 kernel_initializer='he_uniform')) \n",
    "    ann2.add(Dropout(0.2))  # 中等Dropout保持核心特征\n",
    "    \n",
    "    # 输出层（需确认二分类应使用sigmoid）\n",
    "    ann2.add(Dense(num_classes, activation='softmax',\n",
    "                 kernel_initializer='he_uniform'))\n",
    "    \n",
    "    # 编译时增加AUC指标（重要：二分类建议使用binary_crossentropy + sigmoid）\n",
    "    ann2.compile(loss='binary_crossentropy',\n",
    "               optimizer=sgd,  # 可替换为adam进行对比实验\n",
    "               metrics=['accuracy', tensorflow.keras.metrics.AUC(name='auc')])\n",
    "    return ann2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T16:31:24.137841Z",
     "iopub.status.busy": "2025-03-10T16:31:24.137409Z",
     "iopub.status.idle": "2025-03-10T16:31:24.166052Z",
     "shell.execute_reply": "2025-03-10T16:31:24.164909Z",
     "shell.execute_reply.started": "2025-03-10T16:31:24.137765Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Splitter:\n",
    "\tdef __init__(self, test_size=0.2, kfold=True, n_splits=5):\n",
    "\t\t\"\"\"气象预测数据分割器（支持多种分割策略）\n",
    "\n",
    "\t\t参数说明：\n",
    "\t\ttest_size (float): 验证集比例（当kfold=False时生效）\n",
    "\t\tkfold (bool): 启用分层K折交叉验证（保持降水事件分布）\n",
    "\t\tn_splits (int): 交叉验证折数（建议5-10折平衡效率与稳定性）\n",
    "\t\t\"\"\"\n",
    "\t\tself.test_size = test_size  # 验证集保留比例（默认保留20%）\n",
    "\t\tself.kfold = kfold          # 分层抽样开关（针对不平衡降水数据）\n",
    "\t\tself.n_splits = n_splits    # 交叉验证迭代次数\n",
    "\n",
    "\tdef split_data(self, X, y, random_state_list):\n",
    "\t\t\"\"\"生成数据分割迭代器（内存友好型设计）\n",
    "\n",
    "\t\t实现策略：\n",
    "\t\t1. 分层抽样：确保每折中降水/非降水样本比例与全集一致\n",
    "\t\t2. 多重随机：通过random_state_list实现多种随机分割组合\n",
    "\t\t3. 索引保留：使用iloc基于位置索引，避免索引混乱\n",
    "\n",
    "\t\t特别适用于：\n",
    "\t\t- 小样本气象数据（n<10k）\n",
    "\t\t- 高维度特征（特征工程迭代场景）\n",
    "\t\t\"\"\"\n",
    "\t\tif self.kfold:\n",
    "\t\t\t# 分层K折交叉验证循环（保证数据分布一致性）\n",
    "\t\t\tfor random_state in random_state_list:\n",
    "\t\t\t\t# 初始化分层分割器（设置随机种子保障可复现）\n",
    "\t\t\t\tkf = StratifiedKFold(\n",
    "\t\t\t\tn_splits=self.n_splits, \n",
    "\t\t\t\trandom_state=random_state, \n",
    "\t\t\t\tshuffle=True  # 打乱数据避免时序依赖\n",
    "\t\t\t\t)\n",
    "\t\t\t\t# 生成数据索引（内存高效实现）\n",
    "\t\t\t\tfor train_index, val_index in kf.split(X, y):\n",
    "\t\t\t\t\t# 基于位置索引获取数据子集（保留原始索引）\n",
    "\t\t\t\t\tX_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "\t\t\t\t\ty_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\t\t\t\t\t# 生成器返回（支持大数据流式处理）\n",
    "\t\t\t\t\tyield X_train, X_val, y_train, y_val\n",
    "\n",
    "\n",
    "class Classifier:\n",
    "\tdef __init__(self, n_estimators=100, device=\"cpu\", random_state=0):\n",
    "\t\t\"\"\"集成学习分类器工厂类（支持CPU/GPU加速）\n",
    "\n",
    "\t\t参数说明：\n",
    "\t\tn_estimators: int 基学习器数量，影响模型容量和训练时间\n",
    "\t\tdevice: str 计算设备配置，可选'cpu'/'gpu'\n",
    "\t\trandom_state: int 随机种子，确保实验可复现\n",
    "\t\t\"\"\"\n",
    "\t\tself.n_estimators = n_estimators\n",
    "\t\tself.device = device\n",
    "\t\tself.random_state = random_state\n",
    "\t\tself.models = self._define_model()\n",
    "\n",
    "\tdef _define_model(self):\n",
    "\t\t\"\"\"定义集成模型集合（包含梯度提升框架和传统模型）\n",
    "\n",
    "\t\t模型架构策略：\n",
    "\t\t1. 多样性：集成XGBoost/LightGBM/CatBoost三大框架\n",
    "\t\t2. 稳健性：包含随机森林等传统模型作为baseline\n",
    "\t\t3. 领域优化：针对气象数据特性调整正则化参数\n",
    "\t\t\"\"\"\n",
    "\t\t# ================= XGBoost系列 =================\n",
    "\t\t# 基础配置（平衡速度与精度） \n",
    "\t\txgb_params = {\n",
    "\t\t'n_estimators': self.n_estimators,\n",
    "\t\t'learning_rate': 0.05,\n",
    "\t\t'max_depth': 4,\n",
    "\t\t'subsample': 0.8,\n",
    "\t\t'colsample_bytree': 0.1,\n",
    "\t\t'n_jobs': -1,\n",
    "\t\t'eval_metric': 'auc',  # Changed to binary error\n",
    "\t\t'objective': 'binary:logistic',  # Changed to binary objective\n",
    "\t\t'tree_method': 'hist',\n",
    "\t\t'verbosity': 0,\n",
    "\t\t'random_state': self.random_state,\n",
    "\t\t}\n",
    "\n",
    "\t\tif self.device == 'gpu':\n",
    "\t\t\txgb_params['tree_method'] = 'gpu_hist'\n",
    "\t\t\txgb_params['predictor'] = 'gpu_predictor'\n",
    "\n",
    "\t\txgb_params2 = {\n",
    "\t\t'n_estimators': self.n_estimators,\n",
    "\t\t'gamma': 0.279,\n",
    "\t\t'max_depth': 10,\n",
    "\t\t'subsample': 0.325,\n",
    "\t\t'min_child_weight': 9,\n",
    "\t\t'colsample_bytree': 0.487,\n",
    "\t\t'learning_rate': 0.052,\n",
    "\t\t'reg_lambda': 0.0007,\n",
    "\t\t'reg_alpha': 0.371,\n",
    "\t\t'n_jobs': -1,\n",
    "\t\t'eval_metric': 'auc',  # Changed to binary\n",
    "\t\t'objective': 'binary:logistic',  # Changed to binary\n",
    "\t\t'tree_method': 'hist',\n",
    "\t\t'verbosity': 0,\n",
    "\t\t'random_state': self.random_state,\n",
    "\t\t}\n",
    "\n",
    "\t\txgb_params3 = {\n",
    "\t\t'n_estimators': self.n_estimators,\n",
    "\t\t'gamma': 0.279,\n",
    "\t\t'max_depth': 10,\n",
    "\t\t'subsample': 0.325,\n",
    "\t\t'min_child_weight': 9,\n",
    "\t\t'colsample_bytree': 0.487,\n",
    "\t\t'learning_rate': 0.052,\n",
    "\t\t'reg_lambda': 0.0007,\n",
    "\t\t'reg_alpha': 0.371,\n",
    "\t\t'n_jobs': -1,\n",
    "\t\t'eval_metric': 'auc',\n",
    "\t\t'objective': 'binary:logistic',\n",
    "\t\t'tree_method': 'hist',\n",
    "\t\t'verbosity': 0,\n",
    "\t\t'device': 'cuda',\n",
    "\t\t'booster': 'gbtree',\n",
    "\t\t'random_state': self.random_state,\n",
    "\t\t}\n",
    "\n",
    "\t\txgb_params4 = xgb_params.copy()\n",
    "\t\txgb_params4.update({\n",
    "\t\t'subsample': 0.789,\n",
    "\t\t'max_depth': 5,\n",
    "\t\t'learning_rate': 0.161,\n",
    "\t\t'colsample_bytree': 0.243\n",
    "\t\t})\n",
    "\n",
    "\t\txgb_params5 = {\n",
    "\t\t'gamma': 0.3096433389022722, \n",
    "\t\t'max_depth': 13, \n",
    "\t\t'subsample': 0.265592840463903, \n",
    "\t\t'min_child_weight': 5, \n",
    "\t\t'colsample_bytree': 0.0750255657479969, \n",
    "\t\t'learning_rate': 0.014300386108634659, \n",
    "\t\t'reg_lambda': 0.07950866494906, \n",
    "\t\t'reg_alpha': 0.003996021372821422,\n",
    "\t\t'n_jobs': -1,\n",
    "\t\t'eval_metric': 'auc',\n",
    "\t\t'objective': 'binary:logistic',\n",
    "\t\t'tree_method': 'hist',\n",
    "\t\t'verbosity': 0,\n",
    "\t\t'device': 'cuda',\n",
    "\t\t'booster': 'gbtree',\n",
    "\t\t'random_state': self.random_state,}\n",
    "\n",
    "\t\t# ================= LightGBM系列 =================\n",
    "\t\tlgb_params = {\n",
    "\t\t'n_estimators': self.n_estimators,\n",
    "\t\t'max_depth': 10, \n",
    "\t\t'min_samples_leaf': 33, \n",
    "\t\t'subsample': 0.8144362305468624, \n",
    "\t\t'learning_rate': 0.00647777270150904, \n",
    "\t\t'lambda_l1': 1.2991459277687692e-05, \n",
    "\t\t'lambda_l2': 0.0007304768170358017,\n",
    "\t\t'objective': 'binary',  # Changed to binary\n",
    "\t\t'metric': 'auc',  # Changed to binary error\n",
    "\t\t'boosting_type': 'gbdt',\n",
    "\t\t'device': self.device,\n",
    "\t\t'random_state': self.random_state,\n",
    "\t\t'verbose': -1\n",
    "\t\t}\n",
    "\n",
    "\t\tlgb_params2 = {\n",
    "\t\t'n_estimators':self.n_estimators,\n",
    "\t\t'max_depth': 6,\n",
    "\t\t'subsample': 0.743,\n",
    "\t\t'learning_rate': 0.049,\n",
    "\t\t'lambda_l1': 8.922e-05,\n",
    "\t\t'lambda_l2': 0.0018,\n",
    "\t\t'colsample_bytree': 0.392,\n",
    "\t\t'objective': 'binary',\n",
    "\t\t'metric': 'auc',\n",
    "\t\t'boosting_type': 'gbdt',\n",
    "\t\t'device': self.device,\n",
    "\t\t'random_state': self.random_state,\n",
    "\t\t'verbose': -1\n",
    "\t\t}\n",
    "\n",
    "\t\tlgb_params3 = {\n",
    "\t\t'n_estimators': self.n_estimators,\n",
    "\t\t'max_depth': 9,\n",
    "\t\t'subsample': 0.540,\n",
    "\t\t'learning_rate': 0.049,\n",
    "\t\t'lambda_l1': 1.749e-08,\n",
    "\t\t'lambda_l2': 3.837,\n",
    "\t\t'colsample_bytree': 0.319,\n",
    "\t\t'objective': 'binary',\n",
    "\t\t'metric': 'auc',\n",
    "\t\t'boosting_type': 'gbdt',\n",
    "\t\t'device': self.device,\n",
    "\t\t'random_state': self.random_state,\n",
    "\t\t'verbose': -1\n",
    "\t\t}\n",
    "\n",
    "\t\tlgb_params4 = lgb_params2.copy()\n",
    "\t\tlgb_params4.update({\n",
    "\t\t'subsample': 0.9,\n",
    "\t\t'reg_lambda': 0.876,\n",
    "\t\t'reg_alpha': 0.319,\n",
    "\t\t'max_depth': 9,\n",
    "\t\t'learning_rate': 0.107,\n",
    "\t\t'colsample_bytree': 0.1\n",
    "\t\t})\n",
    "\n",
    "\t\tlgb_params5 = lgb_params2.copy()\n",
    "\t\tlgb_params5.update({\n",
    "\t\t'subsample': 0.9,\n",
    "\t\t'reg_lambda': 0.512,\n",
    "\t\t'reg_alpha': 0.898,\n",
    "\t\t'max_depth': 11,\n",
    "\t\t'learning_rate': 0.081,\n",
    "\t\t'colsample_bytree': 0.1\n",
    "\t\t})\n",
    "\n",
    "\t\t# ================= CatBoost系列 =================\n",
    "\t\tcb_params = {\n",
    "\t\t'iterations': self.n_estimators,\n",
    "\t\t'depth': 6,\n",
    "\t\t'learning_rate': 0.05,\n",
    "\t\t'l2_leaf_reg': 0.7,\n",
    "\t\t'random_strength': 0.2,\n",
    "\t\t'max_bin': 200,\n",
    "\t\t'od_wait': 65,\n",
    "\t\t'one_hot_max_size': 70,\n",
    "\t\t'grow_policy': 'Depthwise',\n",
    "\t\t'bootstrap_type': 'Bayesian',\n",
    "\t\t'od_type': 'Iter',\n",
    "\t\t'eval_metric': 'AUC',\n",
    "\t\t'loss_function': 'Logloss',  # Changed to binary\n",
    "\t\t'task_type': self.device.upper(),\n",
    "\t\t'random_state': self.random_state,\n",
    "\t\t'verbose': -1\n",
    "\t\t}\n",
    "\n",
    "\t\tcb_sym_params = cb_params.copy()\n",
    "\t\tcb_sym_params['grow_policy'] = 'SymmetricTree'\n",
    "\n",
    "\t\tcb_loss_params = cb_params.copy()\n",
    "\t\tcb_loss_params['grow_policy'] = 'Lossguide'\n",
    "\n",
    "\t\tcb_params2 = cb_params.copy()\n",
    "\t\tcb_params2.update({\n",
    "\t\t'learning_rate': 0.019,\n",
    "\t\t'depth': 9,\n",
    "\t\t'random_strength': 0.3,\n",
    "\t\t'one_hot_max_size': 10,\n",
    "\t\t'max_bin': 100,\n",
    "\t\t'l2_leaf_reg': 0.419\n",
    "\t\t})\n",
    "\n",
    "\t\tcb_params3 = {\n",
    "\t\t'iterations': self.n_estimators,\n",
    "\t\t'depth': 8, \n",
    "\t\t'learning_rate': 0.010264893665188225, \n",
    "\t\t'l2_leaf_reg': 1.6429322134431932, \n",
    "\t\t'random_strength': 0.7418687826801, \n",
    "\t\t'max_bin': 180, \n",
    "\t\t'one_hot_max_size': 59, \n",
    "\t\t'grow_policy': 'Lossguide', \n",
    "\t\t'od_wait': 80,\n",
    "\t\t'bootstrap_type': 'Bayesian',\n",
    "\t\t'od_type': 'Iter',\n",
    "\t\t'eval_metric': 'AUC',\n",
    "\t\t'loss_function': 'Logloss',\n",
    "\t\t'task_type': self.device.upper(),\n",
    "\t\t'random_state': self.random_state,\n",
    "\t\t}\n",
    "\n",
    "\t\tcb_params4 = cb_params.copy()\n",
    "\t\tcb_params4.update({\n",
    "\t\t'learning_rate': 0.143,\n",
    "\t\t'depth': 16,\n",
    "\t\t'random_strength': 0.596,\n",
    "\t\t# 'one_hot_max_size': 100,\n",
    "\t\t# 'max_bin': 150,\n",
    "\t\t'l2_leaf_reg': 0.384,\n",
    "\t\t'grow_policy': 'Lossguide'\n",
    "\t\t})\n",
    "\n",
    "\t\t# ================= 其他模型配置 =================\n",
    "\t\tdt_params = {'criterion': 'gini', 'max_depth': 9, 'min_samples_split': 17, 'min_samples_leaf': 18, 'max_features': 0.843}\n",
    "\t\tetr_params = {'criterion': 'gini', 'max_depth': 16, 'min_samples_split': 11, 'min_samples_leaf': 1, 'max_features': 0.668, 'bootstrap': True}\n",
    "\t\thist_params = {'learning_rate': 0.058, 'n_iter_no_change': 795, 'max_depth': 4, 'min_samples_leaf': 17, 'max_leaf_nodes': 98, 'l2_regularization': 1.923e-07}\n",
    "\t\trf_params = {'max_depth': 16, 'min_samples_split': 18, 'min_samples_leaf': 2, 'max_features': 0.416}\n",
    "\t\tgbt_params = {'learning_rate': 0.136, 'max_depth': 4, 'min_samples_split': 17, 'min_samples_leaf': 15, 'subsample': 0.886, 'max_features': 0.611}\n",
    "\t\tknn_params = {'n_neighbors': 16, 'weights': 'uniform', 'p': 2, 'leaf_size': 13, 'algorithm': 'ball_tree'}\n",
    "\t\tadb_params = {'n_estimators': 957, 'learning_rate': 0.663}\n",
    "\n",
    "\t\t# ================= 模型集合初始化 =================\n",
    "\t\tmodels = {\n",
    "\t\t'xgb':  xgb.XGBClassifier(**xgb_params),\n",
    "\t\t'xgb2': xgb.XGBClassifier(**xgb_params2),\n",
    "\t\t'xgb3': xgb.XGBClassifier(**xgb_params3),\n",
    "\t\t# 'xgb4': xgb.XGBClassifier(**xgb_params4),\n",
    "\t\t# 'xgb5': xgb.XGBClassifier(**xgb_params5),\n",
    "\t\t'lgb':  lgb.LGBMClassifier(**lgb_params),\n",
    "\t\t'lgb2': lgb.LGBMClassifier(**lgb_params2),\n",
    "\t\t'lgb3': lgb.LGBMClassifier(**lgb_params3),\n",
    "\t\t# 'lgb4': lgb.LGBMClassifier(**lgb_params4),\n",
    "\t\t# 'lgb5': lgb.LGBMClassifier(**lgb_params5),\n",
    "\t\t'cat':  CatBoostClassifier(**cb_params),\n",
    "\t\t# 'cat2': CatBoostClassifier(**cb_params2),\n",
    "\t\t# 'cat3': CatBoostClassifier(**cb_params3),\n",
    "\t\t# 'cat4': CatBoostClassifier(**cb_params4),\n",
    "\t\t\"cat_sym\": CatBoostClassifier(**cb_sym_params),\n",
    "\t\t\"cat_loss\": CatBoostClassifier(**cb_loss_params),\n",
    "\t\t'hist_gbm': HistGradientBoostingClassifier(max_iter=self.n_estimators, **hist_params, random_state=self.random_state),\n",
    "\t\t'rf': RandomForestClassifier(n_estimators=250, **rf_params, random_state=self.random_state),\n",
    "\t\t'gbdt': GradientBoostingClassifier(**gbt_params, n_estimators=1000, random_state=self.random_state),            \n",
    "\t\t'ada': AdaBoostClassifier(**adb_params, random_state=self.random_state),\n",
    "\t\t'etr': ExtraTreesClassifier(**etr_params, random_state=self.random_state),\n",
    "\t\t'dt': DecisionTreeClassifier(**dt_params, random_state=self.random_state),\n",
    "\t\t'knn': KNeighborsClassifier(**knn_params),\n",
    "\t\t'log_reg': LogisticRegression(max_iter=1000),\n",
    "\t\t'ridge': CalibratedClassifierCV(RidgeClassifierCV(alphas=[100.02]), method='sigmoid'),\n",
    "\t\t'elasticNet': LogisticRegressionCV(Cs=[0.044], l1_ratios=[0.977]),\n",
    "\t\t'ann1': init_ann1(1, X_test.shape[1]),  \n",
    "\t\t'ann2': init_ann2(1, X_test.shape[1]),  \n",
    "\t\t}\n",
    "\t\treturn models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T16:31:24.167681Z",
     "iopub.status.busy": "2025-03-10T16:31:24.167335Z",
     "iopub.status.idle": "2025-03-10T16:31:24.19195Z",
     "shell.execute_reply": "2025-03-10T16:31:24.190743Z",
     "shell.execute_reply.started": "2025-03-10T16:31:24.167652Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class OptunaWeights:\n",
    "    def __init__(self, random_state, n_trials=10000):\n",
    "        \"\"\"基于Optuna的模型融合权重优化器\n",
    "        Args:\n",
    "            random_state (int): 随机种子确保可复现性\n",
    "            n_trials (int): 优化过程的总尝试次数，影响搜索空间覆盖率\n",
    "        \"\"\"\n",
    "        self.study = None      # Optuna优化研究实例\n",
    "        self.weights = None    # 最优模型权重列表\n",
    "        self.random_state = random_state\n",
    "        self.n_trials = n_trials\n",
    "\n",
    "    def _objective(self, trial, y_true, y_preds):\n",
    "        \"\"\"优化目标函数：最大化加权预测的AUC分数\n",
    "        Args:\n",
    "            trial (optuna.Trial): Optuna试验对象\n",
    "            y_true (array): 真实标签\n",
    "            y_preds (list): 各模型的预测概率列表\n",
    "        \"\"\"\n",
    "        # 为每个模型生成权重建议值（范围0-1）\n",
    "        weights = [trial.suggest_float(f\"weight{n}\", 0, 1) for n in range(len(y_preds))]\n",
    "        \n",
    "        # 计算加权平均预测概率（加入数值稳定性处理）\n",
    "        weighted_pred = np.average(np.array(y_preds), axis=0, weights=weights)\n",
    "        weighted_pred = weighted_pred/weighted_pred.sum(axis=1, keepdims=True)  # 归一化\n",
    "        \n",
    "        # 使用AUC作为优化指标（气象预测常用指标）\n",
    "        auc_score = roc_auc_score(y_true, weighted_pred)\n",
    "        return auc_score\n",
    "\n",
    "    def fit(self, y_true, y_preds):\n",
    "        \"\"\"执行权重优化过程\n",
    "        Args:\n",
    "            y_true (array): 验证集真实标签\n",
    "            y_preds (list): 各模型在验证集的预测概率\n",
    "        \"\"\"\n",
    "        optuna.logging.set_verbosity(optuna.logging.ERROR)  # 禁用optuna日志\n",
    "        # 配置CMA-ES优化算法（适合连续参数空间）\n",
    "        sampler = optuna.samplers.CmaEsSampler(seed=self.random_state)\n",
    "        # 配置Hyperband剪枝器（提前终止不良试验）\n",
    "        pruner = optuna.pruners.HyperbandPruner()\n",
    "        \n",
    "        # 创建优化研究（最大化目标函数）\n",
    "        self.study = optuna.create_study(\n",
    "            sampler=sampler,\n",
    "            pruner=pruner,\n",
    "            study_name=\"OptunaWeights\",\n",
    "            direction='maximize'\n",
    "        )\n",
    "        \n",
    "        # 执行优化过程\n",
    "        objective_partial = partial(self._objective, y_true=y_true, y_preds=y_preds)\n",
    "        self.study.optimize(objective_partial, n_trials=self.n_trials)\n",
    "        \n",
    "        # 提取最优权重\n",
    "        self.weights = [self.study.best_params[f\"weight{n}\"] for n in range(len(y_preds))]\n",
    "\n",
    "    def predict(self, y_preds):\n",
    "        \"\"\"使用优化后的权重生成最终预测\n",
    "        Args:\n",
    "            y_preds (list): 各模型的测试集预测概率\n",
    "        \"\"\"\n",
    "        assert self.weights is not None, '需先执行fit方法进行权重优化'\n",
    "        return np.average(np.array(y_preds), axis=0, weights=self.weights)\n",
    "\n",
    "    def fit_predict(self, y_true, y_preds):\n",
    "        \"\"\"链式调用：优化权重后直接返回加权预测\"\"\"\n",
    "        self.fit(y_true, y_preds)\n",
    "        return self.predict(y_preds)\n",
    "    \n",
    "    def weights(self):\n",
    "        \"\"\"获取优化后的模型权重\"\"\"\n",
    "        return self.weights\n",
    "\n",
    "def find_best_threshold(y_true, y_pred_probabilities):\n",
    "    \"\"\"动态优化分类阈值（平衡准确率与业务需求）\n",
    "    Args:\n",
    "        y_true (array): 真实标签\n",
    "        y_pred_probabilities (array): 预测概率值\n",
    "    \"\"\"\n",
    "    def objective(threshold):\n",
    "        \"\"\"优化目标：最大化准确率（可替换为F1等其他指标）\"\"\"\n",
    "        y_pred = (y_pred_probabilities >= threshold).astype(int)\n",
    "        return -accuracy_score(y_true, y_pred)  # 负号因为使用最小化优化器\n",
    "    \n",
    "    # 有界优化（0-1范围内搜索最佳阈值）\n",
    "    result = minimize_scalar(objective, bounds=(0, 1), method='bounded')\n",
    "    return result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T16:31:24.193535Z",
     "iopub.status.busy": "2025-03-10T16:31:24.193214Z",
     "iopub.status.idle": "2025-03-10T17:11:24.683231Z",
     "shell.execute_reply": "2025-03-10T17:11:24.682093Z",
     "shell.execute_reply.started": "2025-03-10T16:31:24.193508Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ================= 实验配置 =================\n",
    "kfold = True  # 启用分层K折验证（针对降水事件不平衡分布）\n",
    "n_splits = 1 if not kfold else 10  # 交叉验证折数（平衡计算效率与验证可靠性）\n",
    "random_state = 42  # 基准随机种子（确保实验可复现）\n",
    "random_state_list = [random_state]  # 随机种子集合（支持多种分割组合的鲁棒性验证）\n",
    "n_estimators = 9999  # 最大化基学习器容量（应对气象数据的复杂模式）\n",
    "early_stopping_rounds = 100  # 早停轮数（防止过拟合同时保留重要气象特征）\n",
    "verbose = False  # 关闭冗余日志（节省存储空间）\n",
    "\n",
    "# ================= 初始化组件 =================\n",
    "splitter = Splitter(kfold=kfold, n_splits=n_splits)  # 气象数据专用分割器\n",
    "oof_predss = pd.DataFrame(np.zeros((X_train.shape[0], 1)))  # 存储OOF预测（内存预分配优化）\n",
    "test_predss = np.zeros((X_test.shape[0], 1))  # 存储测试集预测（numpy数组节省内存）\n",
    "ensemble_score = []  # 记录各折集成分数\n",
    "ensemble_acc_score = []  # 记录各折准确率（辅助评估）\n",
    "weights = []  # 存储最优权重组合（用于模型解释）\n",
    "trained_models = {'xgb':[]}  # 模型存档（支持后续特征分析）\n",
    "best_thresholds = []  # 存储最佳分类阈值（应对降水事件不平衡）\n",
    "\n",
    "# ================= 交叉验证循环 =================\n",
    "for i, (X_train_, X_val, y_train_, y_val) in enumerate(splitter.split_data(X_train, y_train, random_state_list=random_state_list)):\n",
    "    # 索引计算（支持多随机种子组合验证）\n",
    "    n = i % n_splits  # 当前折序号\n",
    "    m = i // n_splits  # 随机种子序号\n",
    "    \n",
    "    # 初始化分类器（含气象优化参数）\n",
    "    classifier = Classifier(n_estimators, device, random_state)\n",
    "    models = classifier.models  # 获取集成模型集合\n",
    "    \n",
    "    # 预测结果容器（循环内初始化保障内存安全）\n",
    "    oof_preds = []  # 单折验证集预测\n",
    "    test_preds = []  # 单折测试集预测\n",
    "    start_time_fold = time.time()  # 单折计时开始\n",
    "    \n",
    "    # ================= 模型训练与预测 =================\n",
    "    for name, model in models.items():\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 模型拟合（适配不同框架特性）\n",
    "        if ('xgb' in name) or ('lgb' in name) or ('cat' in name):\n",
    "            # 梯度提升框架（处理气象时序特征）\n",
    "            if 'lgb' in name:\n",
    "                model.fit(X_train_, y_train_, eval_set=[(X_val, y_val)])  # LightGBM早停\n",
    "            elif 'cat' in name:\n",
    "                model.fit(X_train_, y_train_, \n",
    "                         eval_set=[(X_val, y_val)],\n",
    "                         early_stopping_rounds=early_stopping_rounds,\n",
    "                         verbose=verbose)  # CatBoost类别处理\n",
    "            else:\n",
    "                model.fit(X_train_, y_train_, \n",
    "                         eval_set=[(X_val, y_val)],\n",
    "                         verbose=verbose)  # XGBoost GPU加速\n",
    "        elif 'ann' in name:\n",
    "            # 神经网络（捕捉非线性气象模式）\n",
    "            model.fit(X_train_, y_train_,\n",
    "                     validation_data=(X_val, y_val),\n",
    "                     batch_size=16,  # 小批量适应内存限制\n",
    "                     epochs=10,\n",
    "                     verbose=verbose)\n",
    "        else:\n",
    "            # 传统模型（基准对比）\n",
    "            model.fit(X_train_, y_train_)\n",
    "            \n",
    "        # 模型存档（支持事后分析）\n",
    "        if name in trained_models.keys():\n",
    "            trained_models[f'{name}'].append(deepcopy(model))  # 深拷贝避免内存污染\n",
    "            \n",
    "        # ================= 预测生成 =================\n",
    "        if 'ann' in name:\n",
    "            # 神经网络输出处理\n",
    "            test_pred = model.predict(X_test).reshape(-1, 1)\n",
    "            y_val_pred = model.predict(X_val).reshape(-1, 1)\n",
    "        else:\n",
    "            # 概率预测（保留不确定性信息）\n",
    "            test_pred = model.predict_proba(X_test)[:, 1].reshape(-1, 1)\n",
    "            y_val_pred = model.predict_proba(X_val)[:, 1].reshape(-1, 1)\n",
    "        \n",
    "        # 性能评估与记录\n",
    "        end_time = time.time()\n",
    "        time_taken = end_time - start_time\n",
    "        score = roc_auc_score(y_val, y_val_pred)       \n",
    "        print(f'{name} [FOLD-{n} SEED-{random_state_list[m]}] '\n",
    "              f'AUC Score: {score:.5f} '  # 保留5位小数确保精度\n",
    "              f'time taken: {time_taken:.3f} secs')  # 精确到毫秒\n",
    "        \n",
    "        # 结果收集（内存优化设计）\n",
    "        oof_preds.append(y_val_pred)  # 验证集预测暂存\n",
    "        test_preds.append(test_pred)  # 测试集预测暂存\n",
    "    \n",
    "    # ================= 集成优化 =================\n",
    "    optweights = OptunaWeights(random_state=random_state)  # 基于Optuna的权重优化\n",
    "    y_val_pred = optweights.fit_predict(y_val, oof_preds)  # 寻找最优集成权重\n",
    "    oof_predss.loc[X_val.index] = np.array(y_val_pred).reshape(-1, 1)  # 更新OOF预测\n",
    "    \n",
    "    # 集成效果评估\n",
    "    score = roc_auc_score(y_val, y_val_pred)\n",
    "    end_time_fold = time.time()\n",
    "    time_taken = end_time_fold - start_time_fold\n",
    "    \n",
    "    print(f'Ensemble [FOLD-{n} SEED-{random_state_list[m]}] '\n",
    "          f'-------------------> AUC Score: {score:.5f} '\n",
    "          f'fold time taken: {time_taken:.5f} secs')  # 精确时间记录\n",
    "    \n",
    "    # 结果存档（用于后续分析）\n",
    "    ensemble_acc_score.append(score)  # 记录集成分数\n",
    "    weights.append(optweights.weights)  # 保存权重组合\n",
    "    \n",
    "    # 测试集预测集成（加权平均保障稳定性）\n",
    "    test_preds = optweights.predict(test_preds)\n",
    "    test_predss += test_preds / (n_splits * len(random_state_list))  # 多折平均\n",
    "    \n",
    "    gc.collect()  # 显式内存回收（应对大规模气象数据）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-03-10T17:11:24.685095Z",
     "iopub.status.busy": "2025-03-10T17:11:24.684519Z",
     "iopub.status.idle": "2025-03-10T17:11:24.69514Z",
     "shell.execute_reply": "2025-03-10T17:11:24.693837Z",
     "shell.execute_reply.started": "2025-03-10T17:11:24.685049Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ================= 集成效果评估 =================\n",
    "print(f\"\\nEnsemble CV Accuracy: {np.mean(ensemble_acc_score):.5f} ± {np.std(ensemble_acc_score):.5f}\")\n",
    "\n",
    "# ================= 模型权重分析 =================\n",
    "print('--- Model Weights ---')\n",
    "mean_weights = np.mean(weights, axis=0)  # 计算各模型权重均值（反映整体贡献度）\n",
    "std_weights = np.std(weights, axis=0)    # 计算权重标准差（评估模型稳定性）\n",
    "\n",
    "# 遍历模型输出权重统计（保留5位小数确保气象预测精度）\n",
    "for name, mean_weight, std_weight in zip(models.keys(), mean_weights, std_weights):\n",
    "    print(f'{name}: {mean_weight:.5f} ± {std_weight:.5f}')  # 格式示例：xgb: 0.25310 ± 0.01234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T17:11:24.696928Z",
     "iopub.status.busy": "2025-03-10T17:11:24.696503Z",
     "iopub.status.idle": "2025-03-10T17:11:24.738606Z",
     "shell.execute_reply": "2025-03-10T17:11:24.737574Z",
     "shell.execute_reply.started": "2025-03-10T17:11:24.696885Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "oof_predss.to_csv('oof_predss.csv',index=False)\n",
    "\n",
    "submission[target] = test_predss\n",
    "submission.to_csv('submission_pure.csv',index=False)\n",
    "submission\n",
    "# oof_predss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T17:11:24.74017Z",
     "iopub.status.busy": "2025-03-10T17:11:24.739885Z",
     "iopub.status.idle": "2025-03-10T17:11:24.747143Z",
     "shell.execute_reply": "2025-03-10T17:11:24.745873Z",
     "shell.execute_reply.started": "2025-03-10T17:11:24.740145Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def ensemble_mean(sub_list, cols, mean=\"AM\"):\n",
    "    \"\"\"气象预测集成平均算法（支持多种均值策略）\n",
    "    \n",
    "    功能说明：\n",
    "        本函数针对降水概率预测任务，提供三种集成平均方法：\n",
    "        1. 算术平均(AM)：适用于各模型预测质量均衡的场景\n",
    "        2. 几何平均(GM)：强调模型预测的协同效应，降低极端值影响\n",
    "        3. 调和平均(HM)：对低概率预测更敏感，适合优化精确率\n",
    "        \n",
    "    气象数据特性优化：\n",
    "        - 内存安全：使用copy()创建数据副本，避免污染原始预测结果\n",
    "        - 概率保留：直接操作概率值，维持预测不确定性信息\n",
    "        - 多策略支持：适应不同气候模式下的集成需求\n",
    "    \"\"\"\n",
    "    # 深拷贝首个子mission作为基准（内存优化设计）\n",
    "    sub_out = sub_list[0].copy()\n",
    "    \n",
    "    # ================= 算术平均策略 =================\n",
    "    if mean == \"AM\":\n",
    "        for col in cols:\n",
    "            # 算术平均公式：ΣP_i / N （保留所有模型的平均预测概率）\n",
    "            sub_out[col] = sum(df[col] for df in sub_list) / len(sub_list)\n",
    "            \n",
    "    # ================= 几何平均策略 =================\n",
    "    elif mean == \"GM\":\n",
    "        # 初始化乘积基准（应对概率连乘的数值稳定性问题）\n",
    "        for df in sub_list[1:]:\n",
    "            for col in cols:\n",
    "                sub_out[col] *= df[col]  # 累积各模型预测概率\n",
    "        # 几何平均公式：(ΠP_i)^(1/N) （增强模型共识预测的可信度）        \n",
    "        for col in cols:\n",
    "            sub_out[col] = (sub_out[col]) ** (1 / len(sub_list))\n",
    "    \n",
    "    # ================= 调和平均策略 =================\n",
    "    elif mean == \"HM\":\n",
    "        # 调和平均公式：N / Σ(1/P_i) （侧重抑制极端高概率预测）\n",
    "        for col in cols:\n",
    "            sub_out[col] = len(sub_list) / sum(1 / df[col] for df in sub_list)\n",
    "    \n",
    "    return sub_out"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11251744,
     "sourceId": 91714,
     "sourceType": "competition"
    },
    {
     "datasetId": 5651545,
     "sourceId": 9328134,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6823134,
     "sourceId": 11001918,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
